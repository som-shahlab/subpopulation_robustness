{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from prediction_utils.util import df_dict_concat, yaml_read, yaml_write\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(\"/scratch/hdd001/home/haoran/stanford_robustness/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_reload = False\n",
    "pkl_path = Path('df_all.pkl')\n",
    "hparams = ['lr', 'num_hidden', 'drop_prob', 'hidden_dim', 'model_type', 'adjustment_scale', 'lr_lambda']\n",
    "if pkl_path.exists() and not force_reload:\n",
    "    df_all = pd.read_pickle(pkl_path)\n",
    "else:\n",
    "    res = []\n",
    "    for i in tqdm(project_dir.glob('**/result_df_group_standard_eval.parquet')):    \n",
    "        df_i = pd.read_parquet(i)\n",
    "        args_i = json.load((i.parent/'args.json').open('r'))\n",
    "        args_i['task'] = i.parent.parent.name[:-3] + '_' + args_i['label_col']\n",
    "        if args_i['task'] == 'eICUlos3_los3':\n",
    "            args_i['task'] = 'eICUlos_los3'\n",
    "        elif args_i['task'] == 'eICUlos7_los7':\n",
    "            args_i['task'] = 'eICUlos_los7'\n",
    "        \n",
    "        args_i['config_filename'] = i.parent.relative_to(project_dir)\n",
    "        \n",
    "        for j in ['task', 'config_filename', 'group_objective_type', 'selection_metric', \n",
    "                  'balance_groups', 'sensitive_attribute', 'fold_id', 'group_objective_metric', 'subset_attribute',\n",
    "                 'subset_group']:\n",
    "            \n",
    "            if not isinstance(args_i[j], (list, tuple)):\n",
    "                df_i[j] = args_i[j]        \n",
    "        \n",
    "        for hparam in hparams:            \n",
    "            if hparam in args_i:\n",
    "                df_i[hparam] = args_i[hparam]\n",
    "            elif hparam == 'adjustment_scale':\n",
    "                df_i[hparam] = None\n",
    "        df_i['hparams_id'] = (hashlib.md5(str(df_i[hparams + ['task', 'group_objective_type', 'selection_metric', \n",
    "                                          'balance_groups', 'sensitive_attribute', 'group_objective_metric', 'subset_attribute',\n",
    "                                         'subset_group']]\n",
    "                                                   .iloc[0].values.tolist())\n",
    "                                               .encode('utf-8')).hexdigest())            \n",
    "                \n",
    "        if (not args_i['balance_groups'] and args_i['selection_metric'] == 'loss' \n",
    "            and pd.isnull(args_i['subset_attribute']) \n",
    "            and args_i['sensitive_attribute'] == 'gender'\n",
    "            and args_i['group_objective_type'] == 'standard'):\n",
    "            df_i['exp'] = 'erm_baseline'\n",
    "        elif not pd.isnull(args_i['subset_attribute']) and args_i['group_objective_type'] == 'standard':\n",
    "            df_i['exp'] = 'erm_subset'\n",
    "        elif args_i['balance_groups'] and args_i['group_objective_type'] == 'standard':\n",
    "            df_i['exp'] = 'erm_group_aware'\n",
    "        elif args_i['group_objective_type'] == 'dro':\n",
    "            df_i['exp'] = 'dro'\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        res.append(df_i)\n",
    "    df_all = pd.concat(res).reset_index(drop = True)\n",
    "    df_all.to_pickle(pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_df = df_all.drop_duplicates(['config_filename'])[hparams + ['task', 'group_objective_type', 'selection_metric', \n",
    "                                          'balance_groups', 'sensitive_attribute', 'group_objective_metric', 'subset_attribute',\n",
    "                                         'subset_group', 'exp', 'hparams_id', 'fold_id', 'config_filename']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "df_all.drop_duplicates(subset = ['hparams_id', 'fold_id']).groupby('hparams_id')['performance'].count().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[(df_all.exp == 'erm_baseline')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_performance = (\n",
    "    pd.DataFrame(\n",
    "        df\n",
    "        .query('metric == \"loss_bce\" & phase == \"eval\"')\n",
    "        .groupby(['task', 'hparams_id'])\n",
    "        .agg(performance=('performance_overall', 'mean'))\n",
    "        .reset_index()\n",
    "    )\n",
    ")\n",
    "\n",
    "best_model = (\n",
    "    mean_performance.groupby('task')\n",
    "    .agg(performance=('performance','min'))\n",
    "    .merge(mean_performance)   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_config_df_erm = (\n",
    "    best_model[['hparams_id', 'task']]\n",
    "    .merge(config_df[config_df.exp == 'erm_baseline'])\n",
    "    .assign(\n",
    "        tag='erm_baseline'\n",
    "    )\n",
    ")\n",
    "selected_config_df_erm.sensitive_attribute = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_config_df_erm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERM with group aware model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revelant evaluations\n",
    "* Best group-aware model selected by worst-group AUC model selection on the eval set over all hyperparameters\n",
    "* Best group-aware model selected by worst-group loss model selection on the eval set over all hyperparameters\n",
    "* Balanced groups, early stopping on the average loss across groups, model selection on average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[(df_all.exp == 'erm_group_aware')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_df(df, query_str=None):\n",
    "    if query_str is None:\n",
    "        return df\n",
    "    else:\n",
    "        return df.query(query_str)\n",
    "    \n",
    "def select_model_mean_min_max(\n",
    "    df,\n",
    "    metric_name='auc', \n",
    "    agg_func_inner='min', \n",
    "    agg_func_outer='max', \n",
    "    query_str=None,\n",
    "    group_vars=None\n",
    "):\n",
    "    default_group_vars = ['sensitive_attribute', 'eval_attribute']\n",
    "    group_vars = default_group_vars if group_vars is None else default_group_vars + group_vars\n",
    "    mean_performance_by_hparam = (\n",
    "        df\n",
    "        .pipe(query_df, query_str=query_str)\n",
    "        .query('sensitive_attribute == eval_attribute')\n",
    "        .query('metric == @metric_name')\n",
    "        .query('phase == \"eval\"')\n",
    "        .groupby(group_vars + ['config_filename', 'hparams_id', 'task'])\n",
    "        .agg(performance=('performance', agg_func_inner)) \n",
    "        .reset_index()\n",
    "        .groupby(group_vars + ['hparams_id', 'task'])\n",
    "        .agg(performance=('performance', 'mean'))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Get the hparam_id with the best mean performance\n",
    "    return (\n",
    "        mean_performance_by_hparam\n",
    "        .groupby(group_vars + ['task'])\n",
    "        .agg(performance=('performance', agg_func_outer))\n",
    "        .merge(mean_performance_by_hparam)\n",
    "        .drop_duplicates(subset = group_vars + ['performance'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean_auc_min = select_model_mean_min_max(\n",
    "    df,\n",
    "    metric_name='auc',\n",
    "    agg_func_inner='min',\n",
    "    agg_func_outer='max',\n",
    ")\n",
    "\n",
    "best_mean_auc_min = best_mean_auc_min.assign(\n",
    "    config_selection='auc_min_max', \n",
    "    tag='aware_auc_min'\n",
    ")\n",
    "display(best_mean_auc_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean_loss_max = select_model_mean_min_max(\n",
    "    df,\n",
    "    metric_name='loss_bce',\n",
    "    agg_func_inner='max',\n",
    "    agg_func_outer='min'\n",
    ")\n",
    "best_mean_loss_max = best_mean_loss_max.assign(\n",
    "    config_selection='loss_max_min',\n",
    "    tag='aware_loss_max'\n",
    ")\n",
    "display(best_mean_loss_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean_loss_mean_balanced = select_model_mean_min_max(\n",
    "    df,\n",
    "    metric_name='loss_bce',\n",
    "    agg_func_inner='mean',\n",
    "    agg_func_outer='min',\n",
    "    query_str='balance_groups == True & selection_metric == \"loss\"'\n",
    ")\n",
    "best_mean_loss_mean_balanced = best_mean_loss_mean_balanced.assign(\n",
    "    config_selection='loss_mean_min_balanced',\n",
    "    tag='aware_balanced'\n",
    ")\n",
    "display(best_mean_loss_mean_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models_aware = (\n",
    "    pd.concat(\n",
    "        [best_mean_auc_min, best_mean_loss_max, best_mean_loss_mean_balanced]\n",
    "    )\n",
    "    .drop(columns='performance')\n",
    "    .merge(config_df[config_df.exp == 'erm_group_aware'])\n",
    ")\n",
    "selected_models_aware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[(df_all.exp == 'erm_subset')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_performance_subset = (\n",
    "    df\n",
    "    .query('subset_attribute == eval_attribute & subset_group == subset_group')\n",
    "    .query('metric == \"loss_bce\" & phase == \"eval\"')\n",
    "    .groupby(['task', 'hparams_id', 'subset_attribute', 'subset_group'])\n",
    "    .agg(performance=('performance', 'mean'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "best_performance_subset = (\n",
    "    mean_performance_subset\n",
    "    .groupby(['task', 'subset_attribute', 'subset_group'])\n",
    "    .agg(performance=('performance', 'min'))\n",
    "    .reset_index()\n",
    "    .merge(mean_performance_subset)\n",
    "    .drop_duplicates(subset = ['task', 'subset_attribute', 'subset_group', 'performance'])\n",
    ")\n",
    "display(best_performance_subset)\n",
    "\n",
    "selected_models_subset = (\n",
    "    best_performance_subset\n",
    "    .merge(config_df[config_df.exp == 'erm_subset'])\n",
    "    .drop(columns='performance')\n",
    "    .assign(tag='erm_subset')\n",
    ")\n",
    "display(selected_models_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group DRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant comparisons\n",
    "\n",
    "    * \"Best DRO\" - min/max auc model selection\n",
    "    * \"Best DRO\" - max/min loss model selection\n",
    "    * By objective - min/max auc model selection\n",
    "    * By objective - max/min loss model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[df_all.exp == 'dro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean_auc_min_dro = select_model_mean_min_max(\n",
    "    df,\n",
    "    metric_name='auc',\n",
    "    agg_func_inner='min',\n",
    "    agg_func_outer='max',\n",
    ")\n",
    "\n",
    "best_mean_auc_min_dro = best_mean_auc_min_dro.assign(\n",
    "    config_selection='auc_min_max', \n",
    "    tag='dro_auc_min'\n",
    ")\n",
    "display(best_mean_auc_min_dro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean_loss_max_dro = select_model_mean_min_max(\n",
    "    df,\n",
    "    metric_name='loss_bce',\n",
    "    agg_func_inner='max',\n",
    "    agg_func_outer='min'\n",
    ")\n",
    "best_mean_loss_max_dro = best_mean_loss_max_dro.assign(\n",
    "    config_selection='loss_max_min',\n",
    "    tag='dro_loss_max'\n",
    ")\n",
    "display(best_mean_loss_max_dro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean_auc_min_dro_by_objective_metric = select_model_mean_min_max(\n",
    "    df,\n",
    "    metric_name='auc',\n",
    "    agg_func_inner='min',\n",
    "    agg_func_outer='max',\n",
    "    group_vars = ['group_objective_metric']\n",
    ")\n",
    "\n",
    "best_mean_auc_min_dro_by_objective_metric = best_mean_auc_min_dro_by_objective_metric.assign(\n",
    "    config_selection='auc_min_max', \n",
    "    tag=lambda x: (\n",
    "        x.apply(\n",
    "            lambda y: 'dro_auc_min_objective_{}'.format(y.group_objective_metric),\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "display(best_mean_auc_min_dro_by_objective_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean_loss_max_dro_by_objective_metric = select_model_mean_min_max(\n",
    "    df,\n",
    "    metric_name='loss_bce',\n",
    "    agg_func_inner='max',\n",
    "    agg_func_outer='min',\n",
    "    group_vars = ['group_objective_metric']\n",
    ")\n",
    "\n",
    "best_mean_loss_max_dro_by_objective_metric = best_mean_loss_max_dro_by_objective_metric.assign(\n",
    "    config_selection='loss_max_min', \n",
    "    tag=lambda x: (\n",
    "        x.apply(\n",
    "            lambda y: 'dro_loss_max_objective_{}'.format(y.group_objective_metric),\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "display(best_mean_loss_max_dro_by_objective_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models_dro = (\n",
    "    pd.concat(\n",
    "        [best_mean_auc_min_dro, best_mean_loss_max_dro, best_mean_auc_min_dro_by_objective_metric, best_mean_loss_max_dro_by_objective_metric]\n",
    "    )\n",
    "    .drop(columns=['performance', 'group_objective_metric'])\n",
    "    .merge(config_df[config_df.exp == 'dro'])\n",
    ")\n",
    "selected_models_dro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put it all together - export configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_config_df = pd.concat(\n",
    "    [\n",
    "        selected_config_df_erm,\n",
    "        selected_models_aware,\n",
    "        selected_models_subset,\n",
    "        selected_models_dro\n",
    "    ]\n",
    ")\n",
    "selected_config_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_config_df.to_csv(\n",
    "    '/scratch/hdd001/home/haoran/stanford_robustness/results/selected_configs.csv',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_config_df.task.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
