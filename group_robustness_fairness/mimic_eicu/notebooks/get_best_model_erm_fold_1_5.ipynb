{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from prediction_utils.util import df_dict_concat, yaml_read, yaml_write\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(\"/scratch/hdd001/home/haoran/stanford_robustness/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_reload = False\n",
    "pkl_path = Path('df_all_erm.pkl')\n",
    "if pkl_path.exists() and not force_reload:\n",
    "    df_all = pd.read_pickle(pkl_path)\n",
    "else:\n",
    "    res = []\n",
    "    for i in tqdm(project_dir.glob('*ERM/**/result_df_group_standard_eval.parquet')):    \n",
    "        df_i = pd.read_parquet(i)\n",
    "        args_i = json.load((i.parent/'args.json').open('r'))\n",
    "        args_i['task'] = i.parent.parent.name[:-3] + '_' + args_i['label_col']\n",
    "        args_i['config_filename'] = i.parent.name\n",
    "        \n",
    "        if (args_i['balance_groups'] or args_i['selection_metric'] != 'loss' \n",
    "            or not pd.isnull(args_i['subset_attribute']) \n",
    "            or args_i['sensitive_attribute'] != 'gender'):\n",
    "            continue\n",
    "        \n",
    "        for j in ['task', 'config_filename', 'group_objective_type', 'selection_metric', \n",
    "                  'balance_groups', 'sensitive_attribute', 'fold_id', 'group_objective_metric', 'subset_attribute',\n",
    "                 'subset_group']:\n",
    "            \n",
    "            if not isinstance(args_i[j], (list, tuple)):\n",
    "                df_i[j] = args_i[j]\n",
    "        \n",
    "        hparams = ['lr', 'num_hidden', 'drop_prob', 'hidden_dim', 'model_type']\n",
    "        for hparam in hparams:\n",
    "            df_i[hparam] = args_i[hparam]\n",
    "        df_i['hparams_id'] = hashlib.md5(str(df_i[hparams].iloc[0].values.tolist()).encode('utf-8')).hexdigest()  \n",
    "            \n",
    "        res.append(df_i)\n",
    "    df_all = pd.concat(res).reset_index(drop = True)\n",
    "    df_all.to_pickle(pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "np.all(df_all[(df_all.phase == 'eval') & (df_all.eval_attribute == 'gender') & (df_all.eval_group == 'M') & (df_all.metric == 'auc')].groupby(['hparams_id', 'task'])['performance'].count() == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_prefix in ['eICUMortality_target', 'MIMICMortality_target']:\n",
    "\n",
    "    result_df_erm = df_all[df_all.task == task_prefix]\n",
    "\n",
    "    mean_performance = (\n",
    "        pd.DataFrame(\n",
    "            result_df_erm\n",
    "            .query('metric == \"loss_bce\" & phase == \"eval\"')\n",
    "            .groupby(['hparams_id'])\n",
    "            .agg(performance=('performance_overall', 'mean'))\n",
    "            .reset_index()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    best_model = (\n",
    "        mean_performance\n",
    "        .agg(performance=('performance','min'))\n",
    "        .merge(mean_performance)   \n",
    "    )\n",
    "\n",
    "   #  display(best_model)\n",
    "\n",
    "    selected_config_df = best_model[['hparams_id']].merge(result_df_erm)\n",
    "    # display(selected_config_df)\n",
    "    selected_config_dict_list = (\n",
    "        selected_config_df[hparams]\n",
    "        .drop_duplicates()\n",
    "        .to_dict('records')\n",
    "    )\n",
    "    assert len(selected_config_dict_list) == 1\n",
    "    selected_config_dict = selected_config_dict_list[0]\n",
    "    selected_config_dict['task'] = task_prefix\n",
    "    print(selected_config_dict)\n",
    "\n",
    "    yaml_write(selected_config_dict, project_dir/f'{task_prefix}_erm_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
